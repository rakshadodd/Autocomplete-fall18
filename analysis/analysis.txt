Put your name and netid here

(1) Run the program BenchmarkForAutocomplete and copy/paste the 
results here this for #matches = 20

(2) Run the program again for #matches = 10000, paste the results, 
and then make any conclusions about how the # matches 
effects the runtime. 
search	size	#match	binary	brute
	456976	10000	0.3217	0.0318
a	17576	10000	0.0047	0.0126
b	17576	10000	0.0043	0.0110
c	17576	10000	0.0049	0.0080
x	17576	10000	0.0050	0.0117
y	17576	10000	0.0051	0.0119
z	17576	10000	0.0045	0.0080
aa	676	10000	0.0001	0.0049
az	676	10000	0.0001	0.0050
za	676	10000	0.0003	0.0043
zz	676	10000	0.0002	0.0047


(3) Copy/paste the code from BruteAutocomplete.topMatches below. 
Explain what the Big-Oh complexity of the entire loop: 
for(Term t : myTerms) {...} 
is in terms of N, the number of elements in myTerms and 
M, the number of terms that match the prefix. 
Assume that every priority-queue operation runs in O(log k) time. 
Explain your answer which should be in terms of N, M, and k.

(4) Explain why the last for loop in BruteAutocomplete.topMatches 
uses a LinkedList (and not an ArrayList) 
AND why the PriorityQueue uses Term.WeightOrder to get 
the top k heaviest matches -- rather than 
using Term.ReverseWeightOrder.


(5) Explain what the runtime of the 
BinarySearchAutocomplete.topMatches code that you 
implemented is by copy/pasting the code below 
and explaining your answer in terms of N, M, and k.


